\section{Proposed System}\label{sec:yourmethod}

\subsection{Overview}
The method that we propose can be viewed as a batch processing system consisting
of four components (Figure \ref{fig:methodoverview}): the \emph{query
transformer}, the \emph{chunker}, the \emph{tokenizer}, and the \emph{matcher}.
Figure \ref{fig:methodoverview} provides an overview.

\begin{figure}[tb]\centering
	\includegraphics[width=.5\textwidth]{img/methodoverview.eps}
  \caption{An instance of the system with 4 chunks and 3 queries.}
  \label{fig:methodoverview}
\end{figure}

First, the \emph{query transformer} analyzes the query and produces a
\emph{token mapping} and set of state tables. Each state table corresponds to
one dPDA which in turn correspond to the individual queries.

The \emph{chunker} splits the XML streams into chunks of approximately equal
size. Individual opening or closing tag (e.g. \verb;<a>;, or \verb;</a>;) are
never split, however the structure of the XML data contained in the individual
chunks can be arbitrary.

The \emph{tokenizer} then compresses each XML chunk into a tokenstream chunk,
using the token mapping generated by the query transformer to map tag names to
token IDs. Connected together, these tokenstream chunk form the tokenstream. In
the \emph{matcher}, the push-down automatons are run in parallel over the
entire tokenstream to identify matching nodes.

The aim of our method is to reduce memory traffic in the matching phase, as the
tokenstream takes up less memory than the original XML stream. Workloads are
divided differently in the tokenizer and in the matcher. Our method is
successful if we achieve \emph{strong scaling} for the tokenizer and \emph{weak
scaling} for the matcher.

\subsection{Query Transformer}
The query transformer takes as input an XPath query which takes the form of a
text file containing one or more XPath expression. The query transformer outputs
two text files: a token mapping and a file containing the state tables of
the dPDAs, one for each query in the input file.

A token mapping is simply a list containing all tag names that appear in any of
the queries. The line number corresponds to the token id for the tag
name on that particular line. These token numbers serve as input alphabet of
the dPDAs.

The dPDAs are generated using the methods presented in \cite{Green2004}: Each
query is turned into an query tree. However, as opposed to
\cite{Green2004}, we build a query tree for each individual query.
Based upon the query tree a non-deterministic automaton is generated which is
then transformed into a deterministic automaton using the standard power set
method described in \cite{Hopcroft2006}.

\subsection{Chunker}
The chunker splits the input XML file into chunks of approximately equal size.
The minimum size of a chunk is determined by dividing the size of the XML
document by the number of available threads in the tokenizer. The first chunk
starts the beginning of the XML stream. The start of the following chunk is
determined by adding the minimum chunk size to the current position and
searching for the next less-than-sign ('\texttt{<}')â€“these offsets accumulate
over all chunks, rendering the last chunk smaller than the minimum chunk size,
but we assume that the effects of this are negligible for our considerations.

\subsection{Tokenizer and Tokenstream}
The tokenizer maps tag names onto corresponding token numbers according to the
token mapping generated by the query transformer: A top-down parser parses the
XML and whenever an opening or closing tag is found, the tag name is compared
with a list of known tag names from the token mapping. For closing tags, the
token number is negated. If $n$ is the number of unique tag names contained in
the input query, all unknown opening (closing) tags, i.e. tag names that appear
in the XML stream but are not contained in the query, are mapped onto the token
number $n+1$ (or $-(n+1)$ in the case of a closing tag).

For example, consider an XML chunk \verb;<a><c></c></a>; and a token mapping
containing only the tag names \verb;a; and \verb;b; in this order. The
aforementioned XML chunk would be translated into the token stream $1, 3, -3,
-1$.

In our implementation, the token numbers are encoded as 16-bit wide signed
integers. This seemed to be reasonable trade-off between the number of different
tag names ($2^{15}-2$) that can be represented and encoding size. Note that the
token size can in principle also be decided dynamically based on the size of the
token mapping.

Each thread also outputs a chunk of the \emph{offset stream} which contains the
character offset for each tag. The offset stream is generated analogous to the
tokenstream and is not shown in \ref{fig:methodoverview} for brevity.

As we implemented our method on a Xeon Phi which is a shared
memory machine, we employed the fork-join paradigm offered by OpenMP to manage
thread allocation and synchronization. Each XML chunk can be tokenized
independently of all the others, and thus there is no need for further
synchronization between the threads when the threads are running.

\subsection{Matcher}
The matcher runs each pushdown automaton in a separate thread. Hence, the
number of threads is equal to the number of queries in the original XPath
query. As soon as a push-down automaton transitions into an accepting state,
the offset of the matching token in the token stream is written into an output
stream. Hence, the character offset of a matching tag in the original XML, can
be found by reading the value in the offset stream at the position found in the
output stream.

Similar to \cite{Ogden2013} we ignore \texttt{CDATA}-sections and comments for
simplicity. The authors of \cite{Ogden2013} suggest extending the transducer to
accommodate the possibility that a chunk starts within a comment or a
\texttt{CDATA}-section. Similarly, in our case, the tokenizer could just output
tokens for the start and end of comments and \texttt{CDATA}-sections.

%Now comes the ``beef'' of the report, where you explain what you
%did. Again, organize it in paragraphs with titles. As in every section
%you start with a very brief overview of the section.

%In this section, structure is very important so one can follow the technical content.

%Mention and cite any external resources that you used including libraries or other code.
