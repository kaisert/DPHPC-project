\section{Introduction}\label{sec:intro}

In this report we describe the results of our project that was part of the
course ''Design of Parallel and High Performance Computing'' at ETH Z\"urich.
The goal of the project was to develop a parallel implementation of an important
algorithm or application, that scales well in a multicore environment. This
report describes an approach to process large XML files or data streams in
parallel using the XPath query language.

XML is most commonly used as an interoperable file exchange format and as such
is used in a wide variety of fields, from documents formats, such as Open
Document Format \todo{ref} and or XHTML \todo{ref} to various kinds of
configuration file formats. All of these applications have in common that XML is
primarily used as a general tool to persist and restore the dynamic state of an
application program.

Following the on-going trend towards big data, however, XML has also been used
to make large amounts of structured data available as high throughput streams.
One such example is Twitter Firehose which makes all user generated tweets
available trough a single XML-Stream\footnote{Twitter has changed the document
format from XML to JSON. We point out that the techniques presented in this
report can easily be adapted to work with almost any structured data exchange
format.}. Twitter Firehose reaches a throughput of up to 3 Gbps.

Most off-the-shelf are single-threaded and tailored towards the applications
mentioned above. Processing structured data at such a high throughput, however,
only becomes possible by leveraging the power of multi processor systems. As
parsing is an inherently sequential process, this requires the careful
orchestration of parallel processors.

Ideally, the data can be split into chunks than can be processed independently.
In our approach we achieve this by using a compression that is agnostic of the
internal structure of the XML data. The XPath query matching is done using the
compressed data.

Our implementation focuses on XML query matching, using XPath. It is written in
C/C++ and our platform of choice is the Intel's Xeon Phi, incorporating 61 cores
and 16 GB of memory. We found a scalable approach to process queries on large
XML files. Our implementation reaches a throughput of about 0.93 GB/s and offers
a big potential for optimization to the architecture at use. 

\mypar{Related work} Main inspiration of our work was \cite{Ogden2013}. Ogden et
al describe an approach to query XML data with the help of pushdown transducers.
Its core idea is to generate mappings of a start state and an end state. For
each chunk one mapping is being computed and then, when each core has finished
processing its part, they are merged according to the matching end and start
states of each chunk.

Optimally to process XML streams, deterministic automatons are used. The
compilation of XPath queries into automatons is discussed in \cite{Green2004}.


%\mypar{Motivation} Nowadays, in the age of the internet of things and social
%media, where a lot and continuous information is being generated and every
%single person has the opportunity to share their thoughts and insights, a huge
%content of valuable information is available. One of these examples is the
%Twitter Firehose, which generates an output of all tweet being made in real
%time. The data stream may reach an output rate of 3 Gbps and more on  its peak,
%therefore to process this data in real time requires high computational power.
%
%To achieve the necessary processing power, one can use a multicore chip, that
%provides several processing units, that work in parallel. While dealing with
%such a datastream or data file in sequential is trivial and straight-forward, an
%implementation in parallel is not as simple. The data formats used are mostly
%context-free languages, such as XML or JSON. To operate on the stream in
%parallel, it must be splitted into similar chunks and distributed to each core.
%Ideally, these chunks are well-formed and self-contained, such that no
%processing unit requires to exchange any information with other units. To
%generate well-formed chunks, parsing and preprocessing the data beforehand
%sequentially is necessary. As this might pose a bottleneck for parallel and
%high-performance query matching on XML data, in our approach we try to avoid any
%preprocessing on the raw data. Thus we had to find a way, in which each unit is
%able to operate on its chunk without any additional information from its peers.

